include required(classpath("application"))
# Cromwell "system" settings
system {
#  # If 'true', a SIGINT will trigger Cromwell to attempt to abort all currently running jobs before exiting
   abort-jobs-on-terminate = true
#
#  # If 'true' then when Cromwell starts up, it tries to restart incomplete workflows
   workflow-restart = true
#
#  # Cromwell will cap the number of running workflows at N
   max-concurrent-workflows = 100
#
#  # Cromwell will launch up to N submitted workflows at a time, regardless of how many open workflow slots exist
   max-workflow-launch-count = 250
#
#  # Number of seconds between workflow launches
   new-workflow-poll-rate = 30
#
#  # Since the WorkflowLogCopyRouter is initialized in code, this is the number of workers
   number-of-workflow-log-copy-workers = 10
#
#  # Default number of cache read workers
   number-of-cache-read-workers = 24
#
#  abort {
#    # These are the default values in Cromwell, in most circumstances there should not be a need to change them.
#
#    # How frequently Cromwell should scan for aborts.
#    scan-frequency: 10 seconds
#
#    # The cache of in-progress aborts. Cromwell will add entries to this cache once a WorkflowActor has been messaged to abort.
#    # If on the next scan an 'Aborting' status is found for a workflow that has an entry in this cache, Cromwell will not ask
#    # the associated WorkflowActor to abort again.
#    cache {
#      enabled: true
#      # Guava cache concurrency.
#      concurrency: 1
#      # How long entries in the cache should live from the time they are added to the cache.
#      ttl: 20 minutes
#      # Maximum number of entries in the cache.
#      size: 100000
#    }
#  }
}
workflow-options {
#  # Directory where to write per workflow logs
#  #workflow-log-dir: "cromwell-workflow-logs"
#
#  # When true, per workflow logs will be deleted after copying
   workflow-log-temporary: true
#
#  # Workflow-failure-mode determines what happens to other calls when a call fails. Can be either ContinueWhilePossible or NoNewCalls.
#  # Can also be overridden in workflow options. Defaults to NoNewCalls. Uncomment to change:
   workflow-failure-mode: "ContinueWhilePossible"
#
   default {
#    # When a workflow type is not provided on workflow submission, this specifies the default type.
     workflow-type: WDL
#
#    # When a workflow type version is not provided on workflow submission, this specifies the default type version.
#    #workflow-type-version: "draft-2"
#
#    # To set a default hog group rather than defaulting to workflow ID:
#    #hogGroup: "static"
  }
}

backend {
  default = slurm
  providers {
    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
      config {
        concurrent-job-limit = 30
    runtime-attributes = """
      Int cpu = 1
      String requested_memory = "5G"
      String? docker
      String? docker_images_dir = "/home/mgarrido/containers/"
      String? job_sample_id
      String? docker_volume
    """
       # exit-code-timeout-seconds = 120
        submit = """
          sbatch \
          --wait-all-nodes=1 \
          ${"-D " + cwd} \
          ${"--job-name=" + job_name} \
          ${"--cpus-per-task=" + cpu} \
          ${"--mem=" + requested_memory} \
          ${"--error=" + err} \
          ${"--output=" + out} \
          --wrap "/usr/bin/env bash ${script}"       
        """
        submit-docker = """
          ml singularity
          # Build the Docker image into a singularity image, using the head node
          DOCKER_NAME=$(sed -e 's/[^A-Za-z0-9._-]/_/g' <<< ${docker})
          IMAGE=${docker_images_dir}/$DOCKER_NAME.sif
          if [ ! -f $IMAGE ]; then
            singularity pull --name "$IMAGE" docker://${docker}
          fi
          # Submit the script to SLURM
          sbatch \
            ${"-D " + cwd} \
            --wait-all-nodes=1 \
            --job-name ${job_name}${"_" + job_sample_id} \
            ${"--cpus-per-task=" + cpu} \
            ${"--mem=" + requested_memory} \
            ${"--error=" + cwd + "/execution/stderr"} \
            ${"--output=" + cwd + "/execution/stdout"} \
            --wrap "singularity exec --bind ${cwd}:${docker_cwd} ${'--bind ' + docker_volume + ':' + docker_volume} $IMAGE ${job_shell} ${script}"
        """
        kill = "scancel ${job_id}"
        check-alive = "[ \"$(squeue -h -j ${job_id})\" ]"
        job-id-regex = "Submitted batch job (\\d+).*"
        filesystems {
           local {
              localization: [ "soft-link", "hard-link", "copy" ]
          caching {
                duplication-strategy: [ "soft-link", "hard-link", "copy" ]
                # Possible values: file, path, path+modtime
                # "file" will compute an md5 hash of the file content.
                # "path" will compute an md5 hash of the file path. This strategy will only be effective if the duplication-strategy (above) is set to "soft-link",
                # in order to allow for the original file path to be hashed.
                # "path+modtime" will compute an md5 hash of the file path and the last modified time. The same conditions as for "path" apply here.
                # Default: file
                hashing-strategy: "path+modtime"
                #hashing-strategy: "file"
                # When true, will check if a sibling file with the same name and the .md5 extension exists, and if it does, use the content of this file as a hash.
                # If false or the md5 does not exist, will proceed with the above-defined hashing strategy.
                check-sibling-md5: true
            }  
          }
        }
      }
    }
  }
}
# HERE FOR CALLCACHING
#database {
#  profile = "slick.jdbc.MySQLProfile$"
#  db {
#    driver = "com.mysql.jdbc.Driver"
#    url = "jdbc:mysql://192.168.150.14/cromwell_dev?rewriteBatchedStatements=true&useSSL=false&serverTimezone=UTC"
#    user = "cromwell"
#    password = "Cbr4B101nf0"
#    connectionTimeout = 5000
#  }
#  # For batch inserts the number of inserts to send to the DB at a time
#  # insert-batch-size = 2000
#}
call-caching {
  enabled = false
  
  # In a multi-user environment this should be false so unauthorized users don't invalidate results for authorized users. 
  invalidate-bad-cache-results = true
  # blacklist-cache {
  #   # The call caching blacklist cache is off by default. This cache is used to blacklist cache hit paths based on the
  #   # prefixes of cache hit paths that Cromwell has previously failed to copy for permissions reasons.
  #   enabled: true
  #   # Guava cache concurrency.
  #   concurrency: 10000
  #   # How long entries in the cache should live from the time of their last access.
  #   ttl: 20 minutes
  #   # Maximum number of entries in the cache.
  #   size: 1000
  # }
}
